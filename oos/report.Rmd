---
author: 'Peder G. Landsverk'
date:  
title: OOS Evaluation 
output: 
   bookdown::pdf_document2
in_header:
   - \usepackage{booktabs}
   - \usepackage{longtable}
   - \usepackage{array}
   - \usepackage{multirow}
   - \usepackage{wrapfig}
   - \usepackage{float}
   - \usepackage{colortbl}
   - \usepackage{pdflscape}
   - \usepackage{tabu}
   - \usepackage{threeparttable}
   - \usepackage{threeparttablex}
   - \usepackage[normalem]{ulem}
   - \usepackage{makecell}
   - \usepackage{xcolor}
toc : false
---

```{R, niter}
niter <- 20
resf <- NULL 
set.seed(1618915)
```

```{R, include = FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      message = FALSE)
knitr::read_chunk('code.R')
options(xtable.comment = FALSE,
        knitr.kable.NA = " - ")
```

```{R}
library(haven)
library(dplyr)
library(evallib)
library(knitr)
library(kableExtra)
library(ggplot2)
```

```{R, prep, include = FALSE}
```

# Reproduksjon av Out Of Sample - resultater fra Hegre et. al. 2013

I det følgende reproduseres resultater fra tabell 2 og figur 4 fra
originalartikkelen. Alle tallene er produsert med nye algoritmer, i et annet
statistikkmiljø (R fremfor STATA). Alle algoritmene er testet grundig, og har
åpen, etterprøvbar kildekode.

## Tabell 2

I originalartikkelen vises ytelse for 10 modeller, gjennom flere ulike
metrikker, over to ulike subset av dataene: Mellom 2001 og 2009, og mellom 2007
og 2009. Her har jeg reprodusert resultatene for modellen `Combined`, for begge
disse tidsperiodene. 

AUC for Incidence er nøyaktig reprodusert. AUC er en metrikk som avhenger av
definisjonen av Recall (senstitivity / TPR) og Fallout (specificity / FPR). At
AUC blir nøyaktig likt viser at algoritmene som er brukt gir ganske nøyaktig de
samme resultatene som i originalartikkelen.

```{R, table_2, results = "asis"}
```

Avvikene i denne tabellen er interessante, nettopp fordi AUC-resultatene blir
helt like. De små avvikene i TPR og FPR kan muligens skyldes avrundingsfeil i
STATA, som ikke gjengis i R. Videre synes det å være noen feilplasserte tall i
originaltabellen: I originalen gjengis det tilsynelatende kun TPR / FPR for
tidsrommet 07-09, med to ulike klassifikasjonsterskler. Etter å ha regnet ut
TPR og FPR for 01-09 synes det som om disse tallene har sneket seg inn under
07-09 der terskelen er 0.5, imens det er de riktige tallene som vises for
terskel 0.3. Konfidensintervallene er regnet ut med en metode fra DeLong,
DeLong, Pearson (1988), som er den samme som brukes i STATA.

Utover dette er det to større avvik i denne tabellen: AUC for Incidence og
Onset. Jeg har ikke klart å finne ut av hva det er som ikke blir riktig når det
gjelder disse tallene. Jeg har undersøkt om min definisjon av Onset og
Termination er den samme som den er i originalarbeidet, noe den er.

\newpage
## Figur 4

Denne figuren viser ROC for perioden 01-09, slik den gjør i originalarbeidet.
Det inntegnede konfidensintervallet er basert på en bootstrap-teknikk.
DeLong-intervaller lar seg såvidt jeg har forstått ikke bruke på punktestimater
som disse, men her er jeg åpen for forslag. 

```{R table_4, include = TRUE, fig.height = 5, fig.width = 5}
```

Det er også viktig å nevne at denne ROC-figuren ikke er i nøyaktig samsvar med
tallene i tabellen over. Dette er fordi man er nødt til å spesifisere et felles
sett med terskler for å kunne estimere en enkelt linje med flere bootstraps,
noe som gjør at tersklene som brukes her, og tersklene som er brukt for å
estimere AUC over, ikke er de samme. AUC for figuren er 
`r plt$results$auc$score`, mot
`r incidence01_09$score` i tabellen.

\newpage

# Oppsummeringsparagraf

Denne finnes på side 258 i originalartikkelen. Jeg er usikker på hvilke av
subsettene som er brukt for å lage tallene; det virker som om det er samsvar
mellom enkelte av statistikkene for 01-09 og enkelte for 07-09. Ellers er
tallene rimelig like, selv om forskjellene som finnes er litt urovekkende.
Dette er i utgangspunktet enkle regnestykker som enkelt bør la seg replisere
nøyaktig om tallene er like. 

Formlene som brukes for å regne ut disse statistikkene er:

```{R, echo = TRUE}
zerosafeDiv <- function(x,y){
   if(y == 0){
      1
   } else {
      x / y
   }
}
```

```{R, echo = TRUE}
#' recall (Også kjent som TPR / sensitivity / hit rate)
recall <- function(confmat){
   zerosafeDiv(confmat[2,2] , sum(confmat[2,]) )
}
```

```{R, echo = TRUE}
#' precision (Også kjent som PPV)
precision <- function(confmat){
   zerosafeDiv(confmat[2,2] , sum(confmat[,2]))
}
```

```{R, echo = TRUE}
#' fallout (Også kjent som FPR / Specificity)
fallout <- function(confmat){
   zerosafeDiv(confmat[1,2] , sum(confmat[1,]))
}
```

Disse brukes på en confusion matrix, som ser slik ut:

```{R}
kable(data.frame(N = c("TN","FP"), P = c("FN","TP")), booktabs = TRUE)
```

## With 01-09
```{R}
diq <- pred_actual
```

```{R, summaryfigures}
```

The TPR or sensitivity of the combined model with p > .50 as the cut- off is 
`r summaryRates$th_05$recall`.
The FPR with p > .50 cutoff is 
`r summaryRates$th_05$fallout`.
The precision rate, or the proportion of country years with positive predic-
tions that are correctly predicted, is 
`r summaryRates$th_05$precision`.
With a p > .30 cutoff, the model has a TPR of 
`r summaryRates$th_03$recall`
, with a false positive rate at 
`r summaryRates$th_03$fallout`.
The precision rate in that case is 
`r summaryRates$th_03$precision`

## With 07-09
```{R}
diq <- subset 
```

```{R, summaryfigures}
```
The TPR or sensitivity of the combined model with p > .50 as the cut- off is 
`r summaryRates$th_05$recall`.
The FPR with p > .50 cutoff is 
`r summaryRates$th_05$fallout`.
The precision rate, or the proportion of country years with positive predic-
tions that are correctly predicted, is 
`r summaryRates$th_05$precision`.
With a p > .30 cutoff, the model has a TPR of 
`r summaryRates$th_03$recall`
, with a false positive rate at 
`r summaryRates$th_03$fallout`.
The precision rate in that case is 
`r summaryRates$th_03$precision`

## Originaltekst:

The predictive performance is quite good. The TPR or sensitivity of the
combined model with p > .50 as the cut- off is 0.63, implying that the model
correctly predicts conflicts in about 16 of the 26 countries that had conflict
in 2009. The FPR with p > .50 cutoff is 0.030—among the 143 non-conflict
countries in 2009, the model only pre- dicts conflict in about four countries.
The precision rate, or the proportion of country years with positive predic-
tions that are correctly predicted, is 0.79. With a p > .30 cutoff, the model
has a TPR of 79.4% corresponding to about 21 of 26 countries, with a false
positive rate at .078 or 11 out of 143 countries. The precision rate in that
case is 0.65. 

# Oppsummering

Alt i alt er samsvaret med originalartikkelen ganske bra, med noen litt
merkelige avvik, som avvikene mellom de enkle metrikkene Fallout og Recall i
originalen og i mitt arbeid. Videre er vi nødt til å finne ut av hvorfor Onset
og Termination ikke blir likt. Jeg er helt sikker på at disse avvikene ikke
skyldes algoritmene som er brukt, men heller kanskje noe forhåndsbearbeiding av
dataene jeg ikke har klart å finne ennå. Jeg har fortsatt ikke klart å finne
STATA-skriptet som ble brukt for å produsere tallene i artikkelen. 
